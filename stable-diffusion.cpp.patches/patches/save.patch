diff --git LICENSE LICENSE
index 56e1e5a..7a6c6fe 100644
--- LICENSE
+++ LICENSE
@@ -18,4 +18,4 @@ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
+SOFTWARE.
diff --git clip.hpp clip.hpp
index 664da58..db30bc4 100644
--- clip.hpp
+++ clip.hpp
@@ -922,4 +922,4 @@ struct CLIPTextModelRunner : public GGMLRunner {
     }
 };
 
-#endif  // __CLIP_HPP__
\ No newline at end of file
+#endif  // __CLIP_HPP__
diff --git ggml_extend.hpp ggml_extend.hpp
index 47fd3a1..41b5944 100644
--- ggml_extend.hpp
+++ ggml_extend.hpp
@@ -20,20 +20,16 @@
 #include <unordered_map>
 #include <vector>
 
-#include "ggml-alloc.h"
-#include "ggml-backend.h"
-#include "ggml.h"
+#include "llama.cpp/ggml-alloc.h"
+#include "llama.cpp/ggml-backend.h"
+#include "llama.cpp/ggml.h"
 
 #ifdef SD_USE_CUBLAS
-#include "ggml-cuda.h"
+#include "llama.cpp/ggml-cuda.h"
 #endif
 
 #ifdef SD_USE_METAL
-#include "ggml-metal.h"
-#endif
-
-#ifdef SD_USE_SYCL
-#include "ggml-sycl.h"
+#include "llama.cpp/ggml-metal.h"
 #endif
 
 #include "rng.hpp"
@@ -94,6 +90,11 @@ __STATIC_INLINE__ ggml_fp16_t ggml_tensor_get_f16(const ggml_tensor* tensor, int
     return *(ggml_fp16_t*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]);
 }
 
+__STATIC_INLINE__ ggml_bf16_t ggml_tensor_get_bf16(const ggml_tensor* tensor, int l, int k = 0, int j = 0, int i = 0) {
+    GGML_ASSERT(tensor->nb[0] == sizeof(ggml_bf16_t));
+    return *(ggml_bf16_t*)((char*)(tensor->data) + i * tensor->nb[3] + j * tensor->nb[2] + k * tensor->nb[1] + l * tensor->nb[0]);
+}
+
 static struct ggml_tensor* get_tensor_from_graph(struct ggml_cgraph* gf, const char* name) {
     struct ggml_tensor* res = NULL;
     for (int i = 0; i < gf->n_nodes; i++) {
@@ -137,11 +138,11 @@ __STATIC_INLINE__ void print_ggml_tensor(struct ggml_tensor* tensor, bool shape_
                         continue;
                     }
                     if (tensor->type == GGML_TYPE_F32) {
-                        printf("  [%d, %d, %d, %d] = %f\n", i, j, k, l, ggml_tensor_get_f32(tensor, l, k, j, i));
+                        printf("  [%d, %d, %d, %d] = %g\n", i, j, k, l, ggml_tensor_get_f32(tensor, l, k, j, i));
                     } else if (tensor->type == GGML_TYPE_F16) {
-                        printf("  [%d, %d, %d, %d] = %i\n", i, j, k, l, ggml_tensor_get_f16(tensor, l, k, j, i));
-                    } else if (tensor->type == GGML_TYPE_I32) {
-                        printf("  [%d, %d, %d, %d] = %i\n", i, j, k, l, ggml_tensor_get_i32(tensor, l, k, j, i));
+                        printf("  [%d, %d, %d, %d] = %g\n", i, j, k, l, ggml_fp16_to_fp32(ggml_tensor_get_f16(tensor, l, k, j, i)));
+                    } else if (tensor->type == GGML_TYPE_BF16) {
+                        printf("  [%d, %d, %d, %d] = %g\n", i, j, k, l, ggml_bf16_to_fp32(ggml_tensor_get_bf16(tensor, l, k, j, i)));
                     }
                     fflush(stdout);
                 }
@@ -757,14 +758,20 @@ __STATIC_INLINE__ void ggml_backend_tensor_get_and_sync(ggml_backend_t backend,
 }
 
 __STATIC_INLINE__ float ggml_backend_tensor_get_f32(ggml_tensor* tensor) {
-    GGML_ASSERT(tensor->type == GGML_TYPE_F32 || tensor->type == GGML_TYPE_F16);
+    GGML_ASSERT(tensor->type == GGML_TYPE_F32 ||
+                tensor->type == GGML_TYPE_F16 ||
+                tensor->type == GGML_TYPE_BF16);
     float value;
     if (tensor->type == GGML_TYPE_F32) {
         ggml_backend_tensor_get(tensor, &value, 0, sizeof(value));
-    } else {  // GGML_TYPE_F16
+    } else if (tensor->type == GGML_TYPE_F16) {  // GGML_TYPE_F16
         ggml_fp16_t f16_value;
         ggml_backend_tensor_get(tensor, &f16_value, 0, sizeof(f16_value));
         value = ggml_fp16_to_fp32(f16_value);
+    } else {  // GGML_TYPE_BF16
+        ggml_bf16_t bf16_value;
+        ggml_backend_tensor_get(tensor, &bf16_value, 0, sizeof(bf16_value));
+        value = ggml_bf16_to_fp32(bf16_value);
     }
     return value;
 }
diff --git model.cpp model.cpp
index 7ab2287..8dd23bc 100644
--- model.cpp
+++ model.cpp
@@ -4,6 +4,7 @@
 #include <set>
 #include <string>
 #include <unordered_map>
+#include "llamafile/llamafile.h"
 #include <vector>
 
 #include "model.h"
@@ -11,14 +12,14 @@
 #include "util.h"
 #include "vocab.hpp"
 
-#include "ggml-alloc.h"
-#include "ggml-backend.h"
-#include "ggml.h"
+#include "llama.cpp/ggml-alloc.h"
+#include "llama.cpp/ggml-backend.h"
+#include "llama.cpp/ggml.h"
 
 #include "stable-diffusion.h"
 
 #ifdef SD_USE_METAL
-#include "ggml-metal.h"
+#include "llama.cpp/ggml-metal.h"
 #endif
 
 #define ST_HEADER_SIZE_LEN 8
@@ -574,6 +575,8 @@ void convert_tensor(void* src,
     } else if (src_type == GGML_TYPE_F32) {
         if (dst_type == GGML_TYPE_F16) {
             ggml_fp32_to_fp16_row((float*)src, (ggml_fp16_t*)dst, n);
+        } else if (dst_type == GGML_TYPE_BF16) {
+            ggml_fp32_to_bf16_row((float*)src, (ggml_bf16_t*)dst, n);
         } else {
             std::vector<float> imatrix(n_per_row, 1.0f);  // dummy importance matrix
             const float* im = imatrix.data();
@@ -582,6 +585,8 @@ void convert_tensor(void* src,
     } else if (dst_type == GGML_TYPE_F32) {
         if (src_type == GGML_TYPE_F16) {
             ggml_fp16_to_fp32_row((ggml_fp16_t*)src, (float*)dst, n);
+        } else if (src_type == GGML_TYPE_BF16) {
+            ggml_bf16_to_fp32_row((ggml_bf16_t*)src, (float*)dst, n);
         } else {
             auto qtype = ggml_internal_get_type_traits(src_type);
             if (qtype.to_float == NULL) {
@@ -604,6 +609,8 @@ void convert_tensor(void* src,
         qtype.to_float(src, (float*)src_data_f32, n);
         if (dst_type == GGML_TYPE_F16) {
             ggml_fp32_to_fp16_row((float*)src_data_f32, (ggml_fp16_t*)dst, n);
+        } else if (dst_type == GGML_TYPE_BF16) {
+            ggml_fp32_to_bf16_row((float*)src_data_f32, (ggml_bf16_t*)dst, n);
         } else {
             std::vector<float> imatrix(n_per_row, 1.0f);  // dummy importance matrix
             const float* im = imatrix.data();
@@ -754,7 +761,12 @@ bool ModelLoader::init_from_gguf_file(const std::string& file_path, const std::s
 
     gguf_context* ctx_gguf_ = NULL;
     ggml_context* ctx_meta_ = NULL;
-    ctx_gguf_               = gguf_init_from_file(file_path.c_str(), {true, &ctx_meta_});
+    struct llamafile * file = llamafile_open_gguf(file_path.c_str(), "rb");
+    if (!file) {
+        LOG_ERROR("failed to open '%s'", file_path.c_str());
+        return false;
+    }
+    ctx_gguf_               = gguf_init_from_file(file, {true, &ctx_meta_});
     if (!ctx_gguf_) {
         LOG_ERROR("failed to open '%s'", file_path.c_str());
         return false;
@@ -791,7 +803,7 @@ ggml_type str_to_ggml_type(const std::string& dtype) {
     if (dtype == "F16") {
         ttype = GGML_TYPE_F16;
     } else if (dtype == "BF16") {
-        ttype = GGML_TYPE_F32;
+        ttype = GGML_TYPE_BF16;
     } else if (dtype == "F32") {
         ttype = GGML_TYPE_F32;
     }
@@ -1671,7 +1683,7 @@ int64_t ModelLoader::get_params_mem_size(ggml_backend_t backend, ggml_type type)
     return mem_size;
 }
 
-bool convert(const char* input_path, const char* vae_path, const char* output_path, sd_type_t output_type) {
+bool convert(const char* input_path, const char* vae_path, const char* output_path, enum sd_type_t output_type) {
     ModelLoader model_loader;
 
     if (!model_loader.init_from_file(input_path)) {
diff --git model.h model.h
index 5bfce30..60fd7a4 100644
--- model.h
+++ model.h
@@ -10,9 +10,9 @@
 #include <tuple>
 #include <vector>
 
-#include "ggml-backend.h"
-#include "ggml.h"
-#include "json.hpp"
+#include "llama.cpp/ggml-backend.h"
+#include "llama.cpp/ggml.h"
+#include "llama.cpp/json.h"
 #include "zip.h"
 
 #define SD_MAX_DIMS 5
@@ -155,4 +155,4 @@ public:
     static std::string load_merges();
     static std::string load_t5_tokenizer_json();
 };
-#endif  // __MODEL_H__
\ No newline at end of file
+#endif  // __MODEL_H__
diff --git stable-diffusion.cpp stable-diffusion.cpp
index c4705db..352adf1 100644
--- stable-diffusion.cpp
+++ stable-diffusion.cpp
@@ -16,13 +16,13 @@
 #include "tae.hpp"
 #include "vae.hpp"
 
-#define STB_IMAGE_IMPLEMENTATION
-#define STB_IMAGE_STATIC
-#include "stb_image.h"
+// #define STB_IMAGE_IMPLEMENTATION
+// #define STB_IMAGE_STATIC
+#include "third_party/stb/stb_image.h"
 
 // #define STB_IMAGE_WRITE_IMPLEMENTATION
 // #define STB_IMAGE_WRITE_STATIC
-// #include "stb_image_write.h"
+// #include "third_party/stb/stb_image_write.h"
 
 const char* model_version_to_str[] = {
     "1.x",
diff --git t5.hpp t5.hpp
index 79109e3..c44b513 100644
--- t5.hpp
+++ t5.hpp
@@ -12,7 +12,7 @@
 
 #include "darts.h"
 #include "ggml_extend.hpp"
-#include "json.hpp"
+#include "llama.cpp/json.h"
 #include "model.h"
 
 // Port from: https://github.com/google/sentencepiece/blob/master/src/unigram_model.h
@@ -453,7 +453,7 @@ public:
 
     struct ggml_tensor* forward(struct ggml_context* ctx, struct ggml_tensor* x) {
         struct ggml_tensor* w = params["weight"];
-        x                     = ggml_rms_norm(ctx, x, eps);
+        x                     = ggml_norm_ext(ctx, x, eps, false);
         x                     = ggml_mul(ctx, x, w);
         return x;
     }
@@ -978,4 +978,4 @@ struct T5Embedder {
     }
 };
 
-#endif  // __T5_HPP__
\ No newline at end of file
+#endif  // __T5_HPP__
diff --git tae.hpp tae.hpp
index 0e03b88..ba26814 100644
--- tae.hpp
+++ tae.hpp
@@ -191,8 +191,8 @@ struct TinyAutoEncoder : public GGMLRunner {
                     ggml_type wtype,
                     bool decoder_only = true)
         : decode_only(decoder_only),
-          taesd(decode_only),
           GGMLRunner(backend, wtype) {
+        taesd = TAESD(decode_only); // [jart] fix ub
         taesd.init(params_ctx, wtype);
     }
 
@@ -248,4 +248,4 @@ struct TinyAutoEncoder : public GGMLRunner {
     }
 };
 
-#endif  // __TAE_HPP__
\ No newline at end of file
+#endif  // __TAE_HPP__
diff --git upscaler.cpp upscaler.cpp
index 2890ad3..0e3f95d 100644
--- upscaler.cpp
+++ upscaler.cpp
@@ -24,10 +24,6 @@ struct UpscalerGGML {
         ggml_backend_metal_log_set_callback(ggml_log_callback_default, nullptr);
         backend = ggml_backend_metal_init();
 #endif
-#ifdef SD_USE_SYCL
-        LOG_DEBUG("Using SYCL backend");
-        backend = ggml_backend_sycl_init(0);
-#endif
 
         if (!backend) {
             LOG_DEBUG("Using CPU backend");
diff --git util.cpp util.cpp
index 5de5ce2..7aa4863 100644
--- util.cpp
+++ util.cpp
@@ -22,11 +22,10 @@
 #include <unistd.h>
 #endif
 
-#include "ggml.h"
+#include "llama.cpp/ggml.h"
 #include "stable-diffusion.h"
 
-#define STB_IMAGE_RESIZE_IMPLEMENTATION
-#include "stb_image_resize.h"
+#include "third_party/stb/stb_image_resize2.h"
 
 bool ends_with(const std::string& str, const std::string& ending) {
     if (str.length() >= ending.length()) {
@@ -287,9 +286,9 @@ sd_image_t* preprocess_id_image(sd_image_t* img) {
     // 1. do resize using stb_resize functions
 
     unsigned char* buf = (unsigned char*)malloc(sizeof(unsigned char) * 3 * size * size);
-    if (!stbir_resize_uint8(img->data, w, h, 0,
-                            buf, size, size, 0,
-                            c)) {
+    if (!stbir_resize_uint8_srgb(img->data, w, h, 0,
+                                 buf, size, size, 0,
+                                 (stbir_pixel_layout)c)) {
         fprintf(stderr, "%s: resize operation failed \n ", __func__);
         return resized;
     }
@@ -669,4 +668,4 @@ std::vector<std::pair<std::string, float>> parse_prompt_attention(const std::str
     }
 
     return res;
-}
\ No newline at end of file
+}
